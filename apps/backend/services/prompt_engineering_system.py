"""
Prompt Engineering System for Multi-Agent Orchestration.

Provides advanced prompt optimization, template management, and dynamic prompt
generation for 100+ specialized agents with context-aware adaptation.
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from enum import Enum
from dataclasses import dataclass, field
from collections import defaultdict
import uuid
import hashlib
import structlog

from pydantic import BaseModel, Field, validator
from jinja2 import Environment, Template, DictLoader, meta
from core.config import get_settings
from services.agent_orchestrator import AgentType

logger = structlog.get_logger(__name__)
settings = get_settings()


class PromptType(str, Enum):
    """Types of prompts in the system."""
    SYSTEM = "system"           # Core system prompts
    TASK = "task"              # Task-specific prompts
    CONTEXT = "context"        # Context injection prompts
    VALIDATION = "validation"  # Validation and quality check prompts
    ERROR_HANDLING = "error_handling"  # Error recovery prompts
    COORDINATION = "coordination"      # Multi-agent coordination prompts
    OPTIMIZATION = "optimization"     # Performance optimization prompts


class PromptTemplate(BaseModel):
    """Structured prompt template with metadata."""
    id: str = Field(..., description="Unique template identifier")
    name: str = Field(..., description="Human-readable template name")
    template_type: PromptType = Field(..., description="Type of prompt template")
    agent_type: Optional[AgentType] = Field(None, description="Target agent type")
    template_content: str = Field(..., description="Jinja2 template content")
    variables: List[str] = Field(default_factory=list, description="Required template variables")
    optional_variables: List[str] = Field(default_factory=list, description="Optional template variables")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
    version: str = Field(default="1.0.0", description="Template version")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    usage_count: int = Field(default=0, description="Number of times template used")
    performance_score: float = Field(default=0.0, description="Performance score (0-1)")
    tags: Set[str] = Field(default_factory=set, description="Searchable tags")
    
    @validator('template_content')
    def validate_template_syntax(cls, v):
        """Validate Jinja2 template syntax."""
        try:\n            env = Environment()\n            env.parse(v)\n            return v\n        except Exception as e:\n            raise ValueError(f\"Invalid template syntax: {str(e)}\")\n    \n    @validator('variables', pre=True, always=True)\n    def extract_variables(cls, v, values):\n        \"\"\"Extract variables from template if not explicitly provided.\"\"\"\n        if v:\n            return v\n        \n        template_content = values.get('template_content', '')\n        if template_content:\n            env = Environment()\n            ast = env.parse(template_content)\n            variables = meta.find_undeclared_variables(ast)\n            return list(variables)\n        \n        return []


@dataclass\nclass PromptExecutionContext:\n    \"\"\"Context for prompt execution with variables and metadata.\"\"\"\n    template_id: str\n    agent_id: str\n    agent_type: AgentType\n    variables: Dict[str, Any] = field(default_factory=dict)\n    context_data: Dict[str, Any] = field(default_factory=dict)\n    execution_metadata: Dict[str, Any] = field(default_factory=dict)\n    correlation_id: Optional[str] = None\n    created_at: datetime = field(default_factory=datetime.utcnow)\n\n\n@dataclass\nclass PromptExecutionResult:\n    \"\"\"Result of prompt execution with metrics.\"\"\"\n    execution_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    template_id: str = field(default=\"\")\n    agent_id: str = field(default=\"\")\n    rendered_prompt: str = field(default=\"\")\n    execution_time_ms: float = field(default=0.0)\n    token_count: int = field(default=0)\n    success: bool = field(default=True)\n    error_message: Optional[str] = field(default=None)\n    quality_score: Optional[float] = field(default=None)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    executed_at: datetime = field(default_factory=datetime.utcnow)\n\n\n@dataclass\nclass PromptOptimizationMetrics:\n    \"\"\"Metrics for prompt optimization.\"\"\"\n    template_id: str\n    total_executions: int = 0\n    successful_executions: int = 0\n    average_execution_time: float = 0.0\n    average_token_count: int = 0\n    average_quality_score: float = 0.0\n    last_optimization: Optional[datetime] = None\n    optimization_history: List[Dict[str, Any]] = field(default_factory=list)\n\n\nclass PromptEngineeringSystem:\n    \"\"\"Advanced prompt engineering system for multi-agent orchestration.\"\"\"\n    \n    def __init__(self):\n        # Template storage and management\n        self.templates: Dict[str, PromptTemplate] = {}\n        self.template_cache: Dict[str, Template] = {}  # Compiled Jinja2 templates\n        self.execution_history: List[PromptExecutionResult] = []\n        self.optimization_metrics: Dict[str, PromptOptimizationMetrics] = {}\n        \n        # Jinja2 environment with custom functions\n        self.jinja_env = Environment(\n            loader=DictLoader({}),\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n        self._register_custom_functions()\n        \n        # Performance optimization\n        self.template_versioning: Dict[str, List[str]] = {}\n        self.a_b_tests: Dict[str, Dict[str, Any]] = {}\n        \n        # Configuration\n        self.max_execution_history = 10000\n        self.optimization_interval = 3600  # seconds\n        self.cache_ttl = 1800  # seconds\n        \n        # Background tasks\n        self.optimization_task: Optional[asyncio.Task] = None\n        self.cleanup_task: Optional[asyncio.Task] = None\n        \n        self.is_initialized = False\n        self.shutdown_event = asyncio.Event()\n        \n    async def initialize(self) -> None:\n        \"\"\"Initialize the prompt engineering system.\"\"\"\n        try:\n            # Load default templates\n            await self._load_default_templates()\n            \n            # Start background tasks\n            self.optimization_task = asyncio.create_task(self._optimization_loop())\n            self.cleanup_task = asyncio.create_task(self._cleanup_loop())\n            \n            self.is_initialized = True\n            logger.info(\"Prompt engineering system initialized\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize prompt engineering system: {str(e)}\")\n            raise\n    \n    def _register_custom_functions(self) -> None:\n        \"\"\"Register custom Jinja2 functions for prompt templates.\"\"\"\n        \n        def format_list(items: List[Any], separator: str = \", \", last_separator: str = \" and \") -> str:\n            \"\"\"Format a list with proper separators.\"\"\"\n            if not items:\n                return \"\"\n            if len(items) == 1:\n                return str(items[0])\n            if len(items) == 2:\n                return f\"{items[0]}{last_separator}{items[1]}\"\n            return f\"{separator.join(str(item) for item in items[:-1])}{last_separator}{items[-1]}\"\n        \n        def truncate_text(text: str, max_length: int = 200, suffix: str = \"...\") -> str:\n            \"\"\"Truncate text to specified length.\"\"\"\n            if len(text) <= max_length:\n                return text\n            return text[:max_length - len(suffix)] + suffix\n        \n        def highlight_keywords(text: str, keywords: List[str], marker: str = \"**\") -> str:\n            \"\"\"Highlight keywords in text.\"\"\"\n            for keyword in keywords:\n                pattern = re.compile(re.escape(keyword), re.IGNORECASE)\n                text = pattern.sub(f\"{marker}{keyword}{marker}\", text)\n            return text\n        \n        def estimate_tokens(text: str) -> int:\n            \"\"\"Rough token count estimation.\"\"\"\n            # Simple approximation: ~4 characters per token\n            return len(text) // 4\n        \n        def current_timestamp() -> str:\n            \"\"\"Current timestamp in ISO format.\"\"\"\n            return datetime.utcnow().isoformat()\n        \n        def format_duration(seconds: float) -> str:\n            \"\"\"Format duration in human-readable format.\"\"\"\n            if seconds < 60:\n                return f\"{seconds:.1f} seconds\"\n            elif seconds < 3600:\n                return f\"{seconds/60:.1f} minutes\"\n            else:\n                return f\"{seconds/3600:.1f} hours\"\n        \n        # Register functions\n        self.jinja_env.globals.update({\n            'format_list': format_list,\n            'truncate_text': truncate_text,\n            'highlight_keywords': highlight_keywords,\n            'estimate_tokens': estimate_tokens,\n            'current_timestamp': current_timestamp,\n            'format_duration': format_duration\n        })\n    \n    async def _load_default_templates(self) -> None:\n        \"\"\"Load default prompt templates for all agent types.\"\"\"\n        default_templates = {\n            # System templates\n            \"system_initialization\": PromptTemplate(\n                id=\"system_initialization\",\n                name=\"Agent System Initialization\",\n                template_type=PromptType.SYSTEM,\n                template_content=\"\"\"\nYou are {{ agent_name }}, a specialized AI agent in a multi-agent orchestration system.\n\n## Your Role\n{{ role_description }}\n\n## Capabilities\n{% for capability in capabilities %}\n- {{ capability }}\n{% endfor %}\n\n## Current Context\n- Project: {{ project_name or \"Unknown\" }}\n- Phase: {{ project_phase or \"Development\" }}\n- Priority: {{ task_priority or \"Medium\" }}\n\n## Coordination Protocol\n- Report status updates regularly\n- Coordinate with other agents when dependencies exist\n- Escalate blockers immediately\n- Maintain quality standards throughout execution\n\nReady to execute tasks with precision and collaboration.\n\"\"\",\n                variables=[\"agent_name\", \"role_description\", \"capabilities\"],\n                tags={\"system\", \"initialization\", \"coordination\"}\n            ),\n            \n            \"task_execution\": PromptTemplate(\n                id=\"task_execution\",\n                name=\"Standard Task Execution\",\n                template_type=PromptType.TASK,\n                template_content=\"\"\"\n## Task: {{ task_title }}\n\n**Objective**: {{ task_objective }}\n\n**Requirements**:\n{% for requirement in requirements %}\n- {{ requirement }}\n{% endfor %}\n\n**Context**:\n{{ context_information }}\n\n{% if dependencies %}\n**Dependencies**:\n{% for dep in dependencies %}\n- {{ dep.name }}: {{ dep.status }}\n{% endfor %}\n{% endif %}\n\n**Success Criteria**:\n{% for criterion in success_criteria %}\n- {{ criterion }}\n{% endfor %}\n\n**Estimated Duration**: {{ estimated_duration | format_duration }}\n**Priority**: {{ priority }}\n\nExecute this task following best practices and report progress regularly.\n\"\"\",\n                variables=[\"task_title\", \"task_objective\", \"requirements\", \"context_information\", \"success_criteria\"],\n                optional_variables=[\"dependencies\", \"estimated_duration\", \"priority\"],\n                tags={\"task\", \"execution\", \"standard\"}\n            ),\n            \n            \"context_injection\": PromptTemplate(\n                id=\"context_injection\",\n                name=\"Context Injection for Agent Coordination\",\n                template_type=PromptType.CONTEXT,\n                template_content=\"\"\"\n## Updated Context\n\n**Project Status**: {{ project_status }}\n**Active Agents**: {{ active_agents | format_list }}\n**Recent Completions**: {{ recent_completions | truncate_text(300) }}\n\n{% if blockers %}\n**Current Blockers**:\n{% for blocker in blockers %}\n- {{ blocker.description }} (Blocking: {{ blocker.affected_agents | format_list }})\n{% endfor %}\n{% endif %}\n\n{% if architectural_decisions %}\n**Recent Architectural Decisions**:\n{% for decision in architectural_decisions %}\n- {{ decision.title }}: {{ decision.summary }}\n{% endfor %}\n{% endif %}\n\n**Integration Points**: {{ integration_points | format_list }}\n**Quality Gates**: {{ quality_gates | format_list }}\n\nAdjust your approach based on this updated context.\n\"\"\",\n                variables=[\"project_status\", \"active_agents\", \"recent_completions\", \"integration_points\", \"quality_gates\"],\n                optional_variables=[\"blockers\", \"architectural_decisions\"],\n                tags={\"context\", \"coordination\", \"status\"}\n            ),\n            \n            \"error_handling\": PromptTemplate(\n                id=\"error_handling\",\n                name=\"Error Recovery and Handling\",\n                template_type=PromptType.ERROR_HANDLING,\n                template_content=\"\"\"\n## Error Recovery Protocol\n\n**Error Type**: {{ error_type }}\n**Error Message**: {{ error_message }}\n**Failed Operation**: {{ failed_operation }}\n\n**Error Context**:\n{{ error_context }}\n\n**Recovery Steps**:\n1. Analyze the error cause\n2. Determine if retry is appropriate\n3. Implement corrective measures\n4. Test the solution\n5. Report resolution or escalate if unresolvable\n\n{% if retry_count %}\n**Retry Attempt**: {{ retry_count }} of {{ max_retries }}\n{% endif %}\n\n{% if similar_errors %}\n**Similar Previous Errors**:\n{% for error in similar_errors %}\n- {{ error.description }} → {{ error.resolution }}\n{% endfor %}\n{% endif %}\n\nProceed with error recovery following the established protocol.\n\"\"\",\n                variables=[\"error_type\", \"error_message\", \"failed_operation\", \"error_context\"],\n                optional_variables=[\"retry_count\", \"max_retries\", \"similar_errors\"],\n                tags={\"error\", \"recovery\", \"troubleshooting\"}\n            ),\n            \n            \"validation_quality_check\": PromptTemplate(\n                id=\"validation_quality_check\",\n                name=\"Validation and Quality Assurance\",\n                template_type=PromptType.VALIDATION,\n                template_content=\"\"\"\n## Quality Validation Required\n\n**Deliverable**: {{ deliverable_name }}\n**Type**: {{ deliverable_type }}\n\n**Quality Criteria**:\n{% for criterion in quality_criteria %}\n- {{ criterion.name }}: {{ criterion.description }} ({{ criterion.weight }}% weight)\n{% endfor %}\n\n**Validation Checklist**:\n{% for check in validation_checklist %}\n- [ ] {{ check }}\n{% endfor %}\n\n**Acceptance Thresholds**:\n- Minimum Quality Score: {{ min_quality_score }}%\n- Maximum Error Rate: {{ max_error_rate }}%\n- Performance Target: {{ performance_target }}\n\n{% if reference_examples %}\n**Reference Examples**:\n{% for example in reference_examples %}\n- {{ example.name }}: {{ example.score }}% quality\n{% endfor %}\n{% endif %}\n\nPerform thorough validation and provide detailed assessment.\n\"\"\",\n                variables=[\"deliverable_name\", \"deliverable_type\", \"quality_criteria\", \"validation_checklist\"],\n                optional_variables=[\"min_quality_score\", \"max_error_rate\", \"performance_target\", \"reference_examples\"],\n                tags={\"validation\", \"quality\", \"assessment\"}\n            )\n        }\n        \n        # Store templates\n        for template in default_templates.values():\n            await self.register_template(template)\n        \n        logger.info(f\"Loaded {len(default_templates)} default templates\")\n    \n    async def register_template(self, template: PromptTemplate) -> None:\n        \"\"\"Register a new prompt template.\"\"\"\n        try:\n            # Compile template to validate syntax\n            compiled = self.jinja_env.from_string(template.template_content)\n            \n            # Store template\n            self.templates[template.id] = template\n            self.template_cache[template.id] = compiled\n            \n            # Initialize optimization metrics\n            self.optimization_metrics[template.id] = PromptOptimizationMetrics(\n                template_id=template.id\n            )\n            \n            # Version tracking\n            if template.id not in self.template_versioning:\n                self.template_versioning[template.id] = []\n            \n            version_key = f\"{template.id}_{template.version}\"\n            if version_key not in self.template_versioning[template.id]:\n                self.template_versioning[template.id].append(version_key)\n            \n            logger.info(\n                \"Template registered\",\n                template_id=template.id,\n                template_type=template.template_type.value,\n                version=template.version\n            )\n            \n        except Exception as e:\n            logger.error(f\"Failed to register template {template.id}: {str(e)}\")\n            raise\n    \n    async def render_prompt(\n        self,\n        template_id: str,\n        context: PromptExecutionContext\n    ) -> PromptExecutionResult:\n        \"\"\"Render a prompt template with provided context.\"\"\"\n        start_time = datetime.utcnow()\n        \n        try:\n            # Get template\n            if template_id not in self.template_cache:\n                raise ValueError(f\"Template {template_id} not found\")\n            \n            template_obj = self.templates[template_id]\n            compiled_template = self.template_cache[template_id]\n            \n            # Validate required variables\n            missing_vars = set(template_obj.variables) - set(context.variables.keys())\n            if missing_vars:\n                raise ValueError(f\"Missing required variables: {missing_vars}\")\n            \n            # Merge context data with variables\n            render_context = {\n                **context.variables,\n                **context.context_data\n            }\n            \n            # Render template\n            rendered_prompt = compiled_template.render(**render_context)\n            \n            # Calculate metrics\n            execution_time = (datetime.utcnow() - start_time).total_seconds() * 1000  # ms\n            token_count = len(rendered_prompt.split())  # Simple approximation\n            \n            # Create result\n            result = PromptExecutionResult(\n                template_id=template_id,\n                agent_id=context.agent_id,\n                rendered_prompt=rendered_prompt,\n                execution_time_ms=execution_time,\n                token_count=token_count,\n                success=True,\n                metadata={\n                    \"agent_type\": context.agent_type.value,\n                    \"correlation_id\": context.correlation_id,\n                    \"template_version\": template_obj.version,\n                    \"variables_used\": list(context.variables.keys())\n                }\n            )\n            \n            # Update usage statistics\n            await self._update_usage_statistics(template_id, result)\n            \n            # Store execution history\n            self.execution_history.append(result)\n            if len(self.execution_history) > self.max_execution_history:\n                self.execution_history.pop(0)\n            \n            logger.debug(\n                \"Prompt rendered successfully\",\n                template_id=template_id,\n                agent_id=context.agent_id,\n                execution_time_ms=execution_time,\n                token_count=token_count\n            )\n            \n            return result\n            \n        except Exception as e:\n            execution_time = (datetime.utcnow() - start_time).total_seconds() * 1000\n            \n            result = PromptExecutionResult(\n                template_id=template_id,\n                agent_id=context.agent_id,\n                execution_time_ms=execution_time,\n                success=False,\n                error_message=str(e),\n                metadata={\n                    \"agent_type\": context.agent_type.value if context.agent_type else \"unknown\",\n                    \"correlation_id\": context.correlation_id\n                }\n            )\n            \n            logger.error(\n                \"Prompt rendering failed\",\n                template_id=template_id,\n                agent_id=context.agent_id,\n                error=str(e)\n            )\n            \n            return result\n    \n    async def _update_usage_statistics(self, template_id: str, result: PromptExecutionResult) -> None:\n        \"\"\"Update template usage statistics.\"\"\"\n        # Update template usage count\n        if template_id in self.templates:\n            self.templates[template_id].usage_count += 1\n            self.templates[template_id].updated_at = datetime.utcnow()\n        \n        # Update optimization metrics\n        if template_id in self.optimization_metrics:\n            metrics = self.optimization_metrics[template_id]\n            metrics.total_executions += 1\n            \n            if result.success:\n                metrics.successful_executions += 1\n                \n                # Update averages\n                total_successful = metrics.successful_executions\n                metrics.average_execution_time = (\n                    (metrics.average_execution_time * (total_successful - 1) + result.execution_time_ms) \n                    / total_successful\n                )\n                \n                metrics.average_token_count = (\n                    (metrics.average_token_count * (total_successful - 1) + result.token_count) \n                    / total_successful\n                )\n                \n                if result.quality_score is not None:\n                    metrics.average_quality_score = (\n                        (metrics.average_quality_score * (total_successful - 1) + result.quality_score) \n                        / total_successful\n                    )\n    \n    async def optimize_template(\n        self,\n        template_id: str,\n        optimization_target: str = \"performance\",  # \"performance\", \"quality\", \"token_efficiency\"\n        min_executions: int = 100\n    ) -> Optional[PromptTemplate]:\n        \"\"\"Optimize a template based on usage metrics.\"\"\"\n        if template_id not in self.templates or template_id not in self.optimization_metrics:\n            return None\n        \n        metrics = self.optimization_metrics[template_id]\n        template = self.templates[template_id]\n        \n        # Check if we have enough data\n        if metrics.total_executions < min_executions:\n            logger.info(f\"Insufficient data for optimization of {template_id}\")\n            return None\n        \n        # Analyze performance patterns\n        recent_executions = [\n            exec_result for exec_result in self.execution_history\n            if exec_result.template_id == template_id\n            and exec_result.executed_at > datetime.utcnow() - timedelta(days=7)\n        ]\n        \n        if not recent_executions:\n            return None\n        \n        # Generate optimization suggestions\n        optimization_suggestions = await self._generate_optimization_suggestions(\n            template, recent_executions, optimization_target\n        )\n        \n        if optimization_suggestions:\n            # Create optimized version\n            optimized_template = await self._apply_optimizations(\n                template, optimization_suggestions\n            )\n            \n            if optimized_template:\n                # Update version\n                version_parts = template.version.split('.')\n                version_parts[-1] = str(int(version_parts[-1]) + 1)\n                optimized_template.version = '.'.join(version_parts)\n                \n                # Register optimized version\n                await self.register_template(optimized_template)\n                \n                # Record optimization history\n                metrics.optimization_history.append({\n                    \"timestamp\": datetime.utcnow().isoformat(),\n                    \"target\": optimization_target,\n                    \"suggestions\": optimization_suggestions,\n                    \"new_version\": optimized_template.version\n                })\n                \n                metrics.last_optimization = datetime.utcnow()\n                \n                logger.info(\n                    \"Template optimized\",\n                    template_id=template_id,\n                    old_version=template.version,\n                    new_version=optimized_template.version,\n                    target=optimization_target\n                )\n                \n                return optimized_template\n        \n        return None\n    \n    async def _generate_optimization_suggestions(\n        self,\n        template: PromptTemplate,\n        executions: List[PromptExecutionResult],\n        target: str\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Generate optimization suggestions based on execution patterns.\"\"\"\n        suggestions = []\n        \n        # Token efficiency optimization\n        if target in [\"token_efficiency\", \"performance\"]:\n            avg_tokens = sum(e.token_count for e in executions) / len(executions)\n            if avg_tokens > 500:  # Threshold for \"too long\"\n                suggestions.append({\n                    \"type\": \"reduce_verbosity\",\n                    \"description\": \"Template generates long outputs, consider reducing verbosity\",\n                    \"action\": \"Add truncation functions or reduce example text\",\n                    \"impact\": \"token_reduction\"\n                })\n        \n        # Performance optimization\n        if target in [\"performance\", \"quality\"]:\n            avg_time = sum(e.execution_time_ms for e in executions) / len(executions)\n            if avg_time > 100:  # ms\n                suggestions.append({\n                    \"type\": \"simplify_template\",\n                    \"description\": \"Template rendering is slow, consider simplifying\",\n                    \"action\": \"Reduce complex Jinja2 operations or loops\",\n                    \"impact\": \"performance_improvement\"\n                })\n        \n        # Quality optimization\n        if target == \"quality\":\n            quality_scores = [e.quality_score for e in executions if e.quality_score is not None]\n            if quality_scores and sum(quality_scores) / len(quality_scores) < 0.8:\n                suggestions.append({\n                    \"type\": \"improve_clarity\",\n                    \"description\": \"Low quality scores detected, improve template clarity\",\n                    \"action\": \"Add more specific instructions or examples\",\n                    \"impact\": \"quality_improvement\"\n                })\n        \n        return suggestions\n    \n    async def _apply_optimizations(\n        self,\n        template: PromptTemplate,\n        suggestions: List[Dict[str, Any]]\n    ) -> Optional[PromptTemplate]:\n        \"\"\"Apply optimization suggestions to create improved template.\"\"\"\n        # This is a simplified implementation\n        # In practice, this would use more sophisticated NLP techniques\n        \n        optimized_content = template.template_content\n        applied_optimizations = []\n        \n        for suggestion in suggestions:\n            if suggestion[\"type\"] == \"reduce_verbosity\":\n                # Simple optimization: add truncation to long sections\n                if \"{{ context_information }}\" in optimized_content:\n                    optimized_content = optimized_content.replace(\n                        \"{{ context_information }}\",\n                        \"{{ context_information | truncate_text(200) }}\"\n                    )\n                    applied_optimizations.append(suggestion[\"type\"])\n            \n            elif suggestion[\"type\"] == \"simplify_template\":\n                # Remove complex loops if they exist\n                # This is a placeholder - real implementation would be more sophisticated\n                applied_optimizations.append(suggestion[\"type\"])\n            \n            elif suggestion[\"type\"] == \"improve_clarity\":\n                # Add more structure to the template\n                if not optimized_content.startswith(\"##\"):\n                    optimized_content = f\"## {template.name}\\n\\n{optimized_content}\"\n                    applied_optimizations.append(suggestion[\"type\"])\n        \n        if applied_optimizations:\n            # Create new optimized template\n            optimized_template = PromptTemplate(\n                id=template.id,\n                name=f\"{template.name} (Optimized)\",\n                template_type=template.template_type,\n                agent_type=template.agent_type,\n                template_content=optimized_content,\n                variables=template.variables,\n                optional_variables=template.optional_variables,\n                metadata={\n                    **template.metadata,\n                    \"optimization_applied\": applied_optimizations,\n                    \"optimized_from\": template.version\n                },\n                tags=template.tags | {\"optimized\"}\n            )\n            \n            return optimized_template\n        \n        return None\n    \n    async def get_template_metrics(self, template_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get comprehensive metrics for a template.\"\"\"\n        if template_id not in self.templates or template_id not in self.optimization_metrics:\n            return None\n        \n        template = self.templates[template_id]\n        metrics = self.optimization_metrics[template_id]\n        \n        # Recent execution analysis\n        recent_executions = [\n            e for e in self.execution_history\n            if e.template_id == template_id\n            and e.executed_at > datetime.utcnow() - timedelta(days=7)\n        ]\n        \n        success_rate = (metrics.successful_executions / metrics.total_executions * 100) if metrics.total_executions > 0 else 0\n        \n        return {\n            \"template_info\": {\n                \"id\": template.id,\n                \"name\": template.name,\n                \"type\": template.template_type.value,\n                \"version\": template.version,\n                \"usage_count\": template.usage_count,\n                \"performance_score\": template.performance_score\n            },\n            \"execution_metrics\": {\n                \"total_executions\": metrics.total_executions,\n                \"successful_executions\": metrics.successful_executions,\n                \"success_rate_percent\": success_rate,\n                \"average_execution_time_ms\": metrics.average_execution_time,\n                \"average_token_count\": metrics.average_token_count,\n                \"average_quality_score\": metrics.average_quality_score\n            },\n            \"recent_activity\": {\n                \"executions_last_7_days\": len(recent_executions),\n                \"last_used\": template.updated_at.isoformat()\n            },\n            \"optimization\": {\n                \"last_optimization\": metrics.last_optimization.isoformat() if metrics.last_optimization else None,\n                \"optimization_count\": len(metrics.optimization_history),\n                \"available_versions\": self.template_versioning.get(template_id, [])\n            }\n        }\n    \n    async def search_templates(\n        self,\n        query: Optional[str] = None,\n        template_type: Optional[PromptType] = None,\n        agent_type: Optional[AgentType] = None,\n        tags: Optional[Set[str]] = None\n    ) -> List[PromptTemplate]:\n        \"\"\"Search templates by various criteria.\"\"\"\n        results = list(self.templates.values())\n        \n        # Filter by template type\n        if template_type:\n            results = [t for t in results if t.template_type == template_type]\n        \n        # Filter by agent type\n        if agent_type:\n            results = [t for t in results if t.agent_type == agent_type or t.agent_type is None]\n        \n        # Filter by tags\n        if tags:\n            results = [t for t in results if tags.issubset(t.tags)]\n        \n        # Text search\n        if query:\n            query_lower = query.lower()\n            results = [\n                t for t in results\n                if query_lower in t.name.lower()\n                or query_lower in t.template_content.lower()\n                or any(query_lower in tag.lower() for tag in t.tags)\n            ]\n        \n        # Sort by usage and performance\n        results.sort(key=lambda t: (t.usage_count, t.performance_score), reverse=True)\n        \n        return results\n    \n    async def _optimization_loop(self) -> None:\n        \"\"\"Background task for template optimization.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                # Find templates that need optimization\n                for template_id, template in self.templates.items():\n                    if template.usage_count >= 100:  # Threshold for optimization consideration\n                        last_opt = self.optimization_metrics[template_id].last_optimization\n                        if not last_opt or (datetime.utcnow() - last_opt).total_seconds() > self.optimization_interval:\n                            await self.optimize_template(template_id)\n                \n                await asyncio.sleep(3600)  # Check every hour\n                \n            except Exception as e:\n                logger.error(f\"Error in optimization loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(300)  # Wait 5 minutes on error\n    \n    async def _cleanup_loop(self) -> None:\n        \"\"\"Background task for cleaning up old execution history.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                # Remove old execution history\n                cutoff_date = datetime.utcnow() - timedelta(days=30)\n                self.execution_history = [\n                    e for e in self.execution_history\n                    if e.executed_at > cutoff_date\n                ]\n                \n                await asyncio.sleep(3600)  # Clean up every hour\n                \n            except Exception as e:\n                logger.error(f\"Error in cleanup loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(3600)\n    \n    async def get_system_status(self) -> Dict[str, Any]:\n        \"\"\"Get system status and metrics.\"\"\"\n        return {\n            \"total_templates\": len(self.templates),\n            \"total_executions\": sum(m.total_executions for m in self.optimization_metrics.values()),\n            \"successful_executions\": sum(m.successful_executions for m in self.optimization_metrics.values()),\n            \"execution_history_size\": len(self.execution_history),\n            \"cache_size\": len(self.template_cache),\n            \"templates_by_type\": {\n                template_type.value: len([\n                    t for t in self.templates.values()\n                    if t.template_type == template_type\n                ]) for template_type in PromptType\n            },\n            \"most_used_templates\": [\n                {\"id\": t.id, \"name\": t.name, \"usage_count\": t.usage_count}\n                for t in sorted(self.templates.values(), key=lambda x: x.usage_count, reverse=True)[:5]\n            ]\n        }\n    \n    async def shutdown(self) -> None:\n        \"\"\"Gracefully shutdown the prompt engineering system.\"\"\"\n        logger.info(\"Shutting down prompt engineering system\")\n        \n        self.shutdown_event.set()\n        \n        # Cancel background tasks\n        for task in [self.optimization_task, self.cleanup_task]:\n            if task:\n                task.cancel()\n        \n        self.is_initialized = False\n        logger.info(\"Prompt engineering system shutdown complete\")\n\n\n# Global prompt engineering system instance\nprompt_engineering_system = PromptEngineeringSystem()\n\n\nasync def get_prompt_engineering_system() -> PromptEngineeringSystem:\n    \"\"\"Get the global prompt engineering system instance.\"\"\"\n    if not prompt_engineering_system.is_initialized:\n        await prompt_engineering_system.initialize()\n    return prompt_engineering_system