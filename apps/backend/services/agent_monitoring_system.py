"""
Agent Monitoring and Metrics System for Multi-Agent Orchestration.

Provides comprehensive monitoring, alerting, and analytics for 100+ agents
with real-time dashboards, performance tracking, and predictive insights.
"""

import asyncio
import json
import time
import statistics
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple, Union, Callable
from enum import Enum
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
import uuid
import structlog
from contextlib import asynccontextmanager

from pydantic import BaseModel, Field
from core.config import get_settings
from services.agent_orchestrator import AgentType, TaskStatus
from services.agent_state_manager import AgentState

logger = structlog.get_logger(__name__)
settings = get_settings()


class MetricType(str, Enum):
    """Types of metrics collected by the monitoring system."""
    # Performance metrics
    EXECUTION_TIME = "execution_time"
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    LATENCY = "latency"
    
    # Resource utilization
    CPU_USAGE = "cpu_usage"
    MEMORY_USAGE = "memory_usage"
    TOKEN_USAGE = "token_usage"
    BANDWIDTH_USAGE = "bandwidth_usage"
    
    # Quality metrics
    SUCCESS_RATE = "success_rate"
    ERROR_RATE = "error_rate"
    QUALITY_SCORE = "quality_score"
    USER_SATISFACTION = "user_satisfaction"
    
    # Operational metrics
    UPTIME = "uptime"
    AVAILABILITY = "availability"
    RELIABILITY = "reliability"
    SCALABILITY = "scalability"
    
    # Business metrics
    TASK_COMPLETION_RATE = "task_completion_rate"
    SLA_COMPLIANCE = "sla_compliance"
    COST_PER_OPERATION = "cost_per_operation"
    ROI = "roi"


class AlertSeverity(str, Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class AlertStatus(str, Enum):
    """Alert status states."""
    ACTIVE = "active"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


@dataclass
class MetricDataPoint:
    """Individual metric data point."""
    metric_type: MetricType
    value: Union[int, float, bool, str]
    timestamp: datetime = field(default_factory=datetime.utcnow)
    agent_id: Optional[str] = None
    agent_type: Optional[AgentType] = None
    tags: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MetricSeries:
    """Time series of metric data points."""
    metric_type: MetricType
    data_points: deque = field(default_factory=lambda: deque(maxlen=1440))  # 24 hours at 1min intervals
    aggregations: Dict[str, float] = field(default_factory=dict)
    last_updated: datetime = field(default_factory=datetime.utcnow)
    
    def add_point(self, value: Union[int, float], timestamp: Optional[datetime] = None) -> None:
        """Add a data point to the series."""
        point = MetricDataPoint(
            metric_type=self.metric_type,
            value=value,
            timestamp=timestamp or datetime.utcnow()
        )
        self.data_points.append(point)
        self.last_updated = datetime.utcnow()
        self._update_aggregations()
    
    def _update_aggregations(self) -> None:
        """Update aggregated statistics."""
        if not self.data_points:
            return
        
        values = [float(dp.value) for dp in self.data_points if isinstance(dp.value, (int, float))]
        if values:
            self.aggregations = {
                \"current\": values[-1],\n                \"min\": min(values),\n                \"max\": max(values),\n                \"avg\": statistics.mean(values),\n                \"median\": statistics.median(values),\n                \"p95\": statistics.quantiles(values, n=20)[-1] if len(values) >= 20 else max(values),\n                \"p99\": statistics.quantiles(values, n=100)[-1] if len(values) >= 100 else max(values)\n            }\n    \n    def get_recent_values(self, minutes: int = 60) -> List[float]:\n        \"\"\"Get values from the last N minutes.\"\"\"\n        cutoff = datetime.utcnow() - timedelta(minutes=minutes)\n        return [\n            float(dp.value) for dp in self.data_points \n            if dp.timestamp >= cutoff and isinstance(dp.value, (int, float))\n        ]\n    \n    def calculate_trend(self, minutes: int = 60) -> str:\n        \"\"\"Calculate trend direction over the last N minutes.\"\"\"\n        values = self.get_recent_values(minutes)\n        if len(values) < 2:\n            return \"stable\"\n        \n        # Simple trend calculation\n        recent_avg = statistics.mean(values[-min(10, len(values)):])\n        older_avg = statistics.mean(values[:min(10, len(values))])\n        \n        change_percent = ((recent_avg - older_avg) / older_avg) * 100 if older_avg != 0 else 0\n        \n        if change_percent > 5:\n            return \"increasing\"\n        elif change_percent < -5:\n            return \"decreasing\"\n        else:\n            return \"stable\"\n\n\n@dataclass\nclass Alert:\n    \"\"\"System alert with metadata and lifecycle tracking.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    title: str = field(default=\"\")\n    description: str = field(default=\"\")\n    severity: AlertSeverity = AlertSeverity.INFO\n    status: AlertStatus = AlertStatus.ACTIVE\n    \n    # Source information\n    source_type: str = field(default=\"system\")  # system, agent, metric, custom\n    source_id: Optional[str] = None\n    metric_type: Optional[MetricType] = None\n    \n    # Alert conditions\n    condition: Dict[str, Any] = field(default_factory=dict)\n    threshold_value: Optional[float] = None\n    current_value: Optional[float] = None\n    \n    # Lifecycle tracking\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    first_occurrence: datetime = field(default_factory=datetime.utcnow)\n    last_occurrence: datetime = field(default_factory=datetime.utcnow)\n    acknowledged_at: Optional[datetime] = None\n    resolved_at: Optional[datetime] = None\n    acknowledged_by: Optional[str] = None\n    \n    # Metadata\n    tags: Set[str] = field(default_factory=set)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    escalation_level: int = 0\n    occurrence_count: int = 1\n    \n    def acknowledge(self, user_id: str) -> None:\n        \"\"\"Acknowledge the alert.\"\"\"\n        self.status = AlertStatus.ACKNOWLEDGED\n        self.acknowledged_at = datetime.utcnow()\n        self.acknowledged_by = user_id\n    \n    def resolve(self) -> None:\n        \"\"\"Resolve the alert.\"\"\"\n        self.status = AlertStatus.RESOLVED\n        self.resolved_at = datetime.utcnow()\n    \n    def escalate(self) -> None:\n        \"\"\"Escalate the alert to the next level.\"\"\"\n        self.escalation_level += 1\n        self.last_occurrence = datetime.utcnow()\n\n\n@dataclass\nclass AgentHealthStatus:\n    \"\"\"Comprehensive health status for an agent.\"\"\"\n    agent_id: str\n    agent_type: AgentType\n    overall_health_score: float = 100.0  # 0-100 scale\n    \n    # Component health scores\n    performance_score: float = 100.0\n    reliability_score: float = 100.0\n    availability_score: float = 100.0\n    quality_score: float = 100.0\n    \n    # Current status\n    current_state: AgentState = AgentState.IDLE\n    last_activity: datetime = field(default_factory=datetime.utcnow)\n    uptime_percentage: float = 100.0\n    \n    # Performance metrics\n    avg_response_time_ms: float = 0.0\n    success_rate_percent: float = 100.0\n    error_count_24h: int = 0\n    tasks_completed_24h: int = 0\n    \n    # Resource utilization\n    cpu_utilization_percent: float = 0.0\n    memory_utilization_percent: float = 0.0\n    token_usage_percent: float = 0.0\n    \n    # Alerts and issues\n    active_alerts: int = 0\n    last_error: Optional[str] = None\n    last_error_time: Optional[datetime] = None\n    \n    # Trends\n    performance_trend: str = \"stable\"  # improving, stable, degrading\n    reliability_trend: str = \"stable\"\n    \n    # Update timestamp\n    updated_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def calculate_overall_health(self) -> None:\n        \"\"\"Calculate overall health score from component scores.\"\"\"\n        # Weighted average of component scores\n        weights = {\n            \"performance\": 0.3,\n            \"reliability\": 0.3,\n            \"availability\": 0.2,\n            \"quality\": 0.2\n        }\n        \n        self.overall_health_score = (\n            self.performance_score * weights[\"performance\"] +\n            self.reliability_score * weights[\"reliability\"] +\n            self.availability_score * weights[\"availability\"] +\n            self.quality_score * weights[\"quality\"]\n        )\n        \n        self.updated_at = datetime.utcnow()\n\n\nclass AlertRule:\n    \"\"\"Configurable alert rule for monitoring conditions.\"\"\"\n    \n    def __init__(\n        self,\n        name: str,\n        metric_type: MetricType,\n        condition: str,  # \">\", \"<\", \">=\", \"<=\", \"==\", \"!=\"\n        threshold: float,\n        severity: AlertSeverity = AlertSeverity.WARNING,\n        evaluation_window_minutes: int = 5,\n        consecutive_breaches: int = 1\n    ):\n        self.name = name\n        self.metric_type = metric_type\n        self.condition = condition\n        self.threshold = threshold\n        self.severity = severity\n        self.evaluation_window_minutes = evaluation_window_minutes\n        self.consecutive_breaches = consecutive_breaches\n        self.current_breach_count = 0\n        self.last_evaluation = datetime.utcnow()\n    \n    def evaluate(self, metric_series: MetricSeries) -> Optional[Alert]:\n        \"\"\"Evaluate the rule against a metric series.\"\"\"\n        recent_values = metric_series.get_recent_values(self.evaluation_window_minutes)\n        \n        if not recent_values:\n            return None\n        \n        current_value = recent_values[-1]\n        breach = self._check_condition(current_value)\n        \n        if breach:\n            self.current_breach_count += 1\n        else:\n            self.current_breach_count = 0\n        \n        # Only create alert if consecutive breaches threshold is met\n        if self.current_breach_count >= self.consecutive_breaches:\n            alert = Alert(\n                title=f\"{self.name} Alert\",\n                description=f\"{self.metric_type.value} {self.condition} {self.threshold}\",\n                severity=self.severity,\n                metric_type=self.metric_type,\n                condition={\n                    \"operator\": self.condition,\n                    \"threshold\": self.threshold,\n                    \"window_minutes\": self.evaluation_window_minutes\n                },\n                threshold_value=self.threshold,\n                current_value=current_value\n            )\n            \n            self.current_breach_count = 0  # Reset counter after creating alert\n            return alert\n        \n        return None\n    \n    def _check_condition(self, value: float) -> bool:\n        \"\"\"Check if the condition is met.\"\"\"\n        if self.condition == \">\":\n            return value > self.threshold\n        elif self.condition == \">=\":\n            return value >= self.threshold\n        elif self.condition == \"<\":\n            return value < self.threshold\n        elif self.condition == \"<=\":\n            return value <= self.threshold\n        elif self.condition == \"==\":\n            return abs(value - self.threshold) < 0.001\n        elif self.condition == \"!=\":\n            return abs(value - self.threshold) >= 0.001\n        else:\n            return False\n\n\nclass AgentMonitoringSystem:\n    \"\"\"Comprehensive monitoring and metrics system for multi-agent orchestration.\"\"\"\n    \n    def __init__(self):\n        # Metrics storage\n        self.agent_metrics: Dict[str, Dict[MetricType, MetricSeries]] = defaultdict(lambda: {})\n        self.system_metrics: Dict[MetricType, MetricSeries] = {}\n        \n        # Health tracking\n        self.agent_health: Dict[str, AgentHealthStatus] = {}\n        \n        # Alerting system\n        self.alert_rules: List[AlertRule] = []\n        self.active_alerts: Dict[str, Alert] = {}  # alert_id -> Alert\n        self.alert_history: deque = deque(maxlen=10000)  # Keep last 10K alerts\n        \n        # Event handlers\n        self.alert_handlers: List[Callable[[Alert], None]] = []\n        \n        # Performance optimization\n        self.metric_buffer: Dict[str, List[MetricDataPoint]] = defaultdict(list)\n        self.buffer_flush_interval = 30  # seconds\n        self.max_buffer_size = 1000\n        \n        # Background tasks\n        self.collection_task: Optional[asyncio.Task] = None\n        self.evaluation_task: Optional[asyncio.Task] = None\n        self.health_task: Optional[asyncio.Task] = None\n        self.cleanup_task: Optional[asyncio.Task] = None\n        self.dashboard_task: Optional[asyncio.Task] = None\n        \n        # Configuration\n        self.collection_interval = 60  # seconds\n        self.evaluation_interval = 60  # seconds\n        self.health_check_interval = 120  # seconds\n        self.cleanup_interval = 3600  # seconds\n        \n        # System state\n        self.start_time = datetime.utcnow()\n        self.is_running = False\n        self.shutdown_event = asyncio.Event()\n        \n    async def initialize(self) -> None:\n        \"\"\"Initialize the monitoring system.\"\"\"\n        try:\n            # Initialize system metrics\n            await self._initialize_system_metrics()\n            \n            # Set up default alert rules\n            await self._setup_default_alert_rules()\n            \n            # Start background tasks\n            self.collection_task = asyncio.create_task(self._collection_loop())\n            self.evaluation_task = asyncio.create_task(self._evaluation_loop())\n            self.health_task = asyncio.create_task(self._health_check_loop())\n            self.cleanup_task = asyncio.create_task(self._cleanup_loop())\n            self.dashboard_task = asyncio.create_task(self._dashboard_update_loop())\n            \n            self.is_running = True\n            logger.info(\"Agent monitoring system initialized\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize monitoring system: {str(e)}\")\n            raise\n    \n    async def _initialize_system_metrics(self) -> None:\n        \"\"\"Initialize system-wide metrics.\"\"\"\n        system_metric_types = [\n            MetricType.THROUGHPUT,\n            MetricType.LATENCY,\n            MetricType.SUCCESS_RATE,\n            MetricType.ERROR_RATE,\n            MetricType.AVAILABILITY,\n            MetricType.RELIABILITY\n        ]\n        \n        for metric_type in system_metric_types:\n            self.system_metrics[metric_type] = MetricSeries(metric_type=metric_type)\n        \n        logger.info(f\"Initialized {len(system_metric_types)} system metrics\")\n    \n    async def _setup_default_alert_rules(self) -> None:\n        \"\"\"Set up default alert rules for common scenarios.\"\"\"\n        default_rules = [\n            AlertRule(\n                name=\"High Error Rate\",\n                metric_type=MetricType.ERROR_RATE,\n                condition=\">\",\n                threshold=10.0,  # 10% error rate\n                severity=AlertSeverity.WARNING,\n                evaluation_window_minutes=5\n            ),\n            AlertRule(\n                name=\"Critical Error Rate\",\n                metric_type=MetricType.ERROR_RATE,\n                condition=\">\",\n                threshold=25.0,  # 25% error rate\n                severity=AlertSeverity.CRITICAL,\n                evaluation_window_minutes=5\n            ),\n            AlertRule(\n                name=\"Low Success Rate\",\n                metric_type=MetricType.SUCCESS_RATE,\n                condition=\"<\",\n                threshold=90.0,  # 90% success rate\n                severity=AlertSeverity.WARNING,\n                evaluation_window_minutes=10\n            ),\n            AlertRule(\n                name=\"High Response Time\",\n                metric_type=MetricType.RESPONSE_TIME,\n                condition=\">\",\n                threshold=5000.0,  # 5 seconds\n                severity=AlertSeverity.WARNING,\n                evaluation_window_minutes=5\n            ),\n            AlertRule(\n                name=\"Critical Response Time\",\n                metric_type=MetricType.RESPONSE_TIME,\n                condition=\">\",\n                threshold=10000.0,  # 10 seconds\n                severity=AlertSeverity.CRITICAL,\n                evaluation_window_minutes=3\n            ),\n            AlertRule(\n                name=\"Low Availability\",\n                metric_type=MetricType.AVAILABILITY,\n                condition=\"<\",\n                threshold=99.0,  # 99% availability\n                severity=AlertSeverity.ERROR,\n                evaluation_window_minutes=15\n            )\n        ]\n        \n        self.alert_rules.extend(default_rules)\n        logger.info(f\"Set up {len(default_rules)} default alert rules\")\n    \n    async def register_agent(self, agent_id: str, agent_type: AgentType) -> None:\n        \"\"\"Register an agent for monitoring.\"\"\"\n        # Initialize metrics for the agent\n        metric_types = [\n            MetricType.EXECUTION_TIME,\n            MetricType.RESPONSE_TIME,\n            MetricType.SUCCESS_RATE,\n            MetricType.ERROR_RATE,\n            MetricType.CPU_USAGE,\n            MetricType.MEMORY_USAGE,\n            MetricType.TOKEN_USAGE,\n            MetricType.QUALITY_SCORE\n        ]\n        \n        for metric_type in metric_types:\n            self.agent_metrics[agent_id][metric_type] = MetricSeries(metric_type=metric_type)\n        \n        # Initialize health status\n        self.agent_health[agent_id] = AgentHealthStatus(\n            agent_id=agent_id,\n            agent_type=agent_type\n        )\n        \n        logger.info(f\"Agent {agent_id} registered for monitoring\")\n    \n    async def record_metric(\n        self,\n        agent_id: str,\n        metric_type: MetricType,\n        value: Union[int, float],\n        timestamp: Optional[datetime] = None,\n        tags: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"Record a metric value for an agent.\"\"\"\n        metric_point = MetricDataPoint(\n            metric_type=metric_type,\n            value=value,\n            timestamp=timestamp or datetime.utcnow(),\n            agent_id=agent_id,\n            tags=tags or {}\n        )\n        \n        # Buffer the metric for batch processing\n        buffer_key = f\"{agent_id}_{metric_type.value}\"\n        self.metric_buffer[buffer_key].append(metric_point)\n        \n        # Flush buffer if it gets too large\n        if len(self.metric_buffer[buffer_key]) >= self.max_buffer_size:\n            await self._flush_metric_buffer(buffer_key)\n    \n    async def record_system_metric(\n        self,\n        metric_type: MetricType,\n        value: Union[int, float],\n        timestamp: Optional[datetime] = None\n    ) -> None:\n        \"\"\"Record a system-wide metric.\"\"\"\n        if metric_type in self.system_metrics:\n            self.system_metrics[metric_type].add_point(value, timestamp)\n    \n    async def _flush_metric_buffer(self, buffer_key: str) -> None:\n        \"\"\"Flush buffered metrics to storage.\"\"\"\n        if buffer_key not in self.metric_buffer or not self.metric_buffer[buffer_key]:\n            return\n        \n        # Parse buffer key\n        agent_id, metric_type_str = buffer_key.rsplit('_', 1)\n        metric_type = MetricType(metric_type_str)\n        \n        # Add points to metric series\n        if agent_id in self.agent_metrics and metric_type in self.agent_metrics[agent_id]:\n            series = self.agent_metrics[agent_id][metric_type]\n            for point in self.metric_buffer[buffer_key]:\n                series.add_point(point.value, point.timestamp)\n        \n        # Clear buffer\n        self.metric_buffer[buffer_key].clear()\n    \n    async def _flush_all_buffers(self) -> None:\n        \"\"\"Flush all metric buffers.\"\"\"\n        for buffer_key in list(self.metric_buffer.keys()):\n            await self._flush_metric_buffer(buffer_key)\n    \n    async def _collection_loop(self) -> None:\n        \"\"\"Background task for metric collection.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                await self._flush_all_buffers()\n                await self._collect_system_metrics()\n                await asyncio.sleep(self.collection_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in collection loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(30)\n    \n    async def _collect_system_metrics(self) -> None:\n        \"\"\"Collect system-wide metrics.\"\"\"\n        try:\n            # Calculate system-wide success rate\n            total_success = 0\n            total_attempts = 0\n            \n            for agent_id, metrics in self.agent_metrics.items():\n                if MetricType.SUCCESS_RATE in metrics:\n                    success_rate_series = metrics[MetricType.SUCCESS_RATE]\n                    recent_values = success_rate_series.get_recent_values(60)\n                    if recent_values:\n                        total_success += recent_values[-1]\n                        total_attempts += 100  # Assuming success rate is percentage\n            \n            if total_attempts > 0:\n                system_success_rate = (total_success / total_attempts) * 100\n                await self.record_system_metric(MetricType.SUCCESS_RATE, system_success_rate)\n            \n            # Calculate system availability\n            active_agents = len([h for h in self.agent_health.values() if h.current_state != AgentState.OFFLINE])\n            total_agents = len(self.agent_health)\n            \n            if total_agents > 0:\n                availability = (active_agents / total_agents) * 100\n                await self.record_system_metric(MetricType.AVAILABILITY, availability)\n            \n        except Exception as e:\n            logger.error(f\"Error collecting system metrics: {str(e)}\", exc_info=True)\n    \n    async def _evaluation_loop(self) -> None:\n        \"\"\"Background task for alert rule evaluation.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                await self._evaluate_alert_rules()\n                await asyncio.sleep(self.evaluation_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in evaluation loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(30)\n    \n    async def _evaluate_alert_rules(self) -> None:\n        \"\"\"Evaluate all alert rules against current metrics.\"\"\"\n        for rule in self.alert_rules:\n            try:\n                # Evaluate against system metrics\n                if rule.metric_type in self.system_metrics:\n                    alert = rule.evaluate(self.system_metrics[rule.metric_type])\n                    if alert:\n                        await self._handle_alert(alert)\n                \n                # Evaluate against agent metrics\n                for agent_id, metrics in self.agent_metrics.items():\n                    if rule.metric_type in metrics:\n                        alert = rule.evaluate(metrics[rule.metric_type])\n                        if alert:\n                            alert.source_id = agent_id\n                            alert.tags.add(f\"agent:{agent_id}\")\n                            await self._handle_alert(alert)\n                            \n            except Exception as e:\n                logger.error(f\"Error evaluating rule {rule.name}: {str(e)}\", exc_info=True)\n    \n    async def _handle_alert(self, alert: Alert) -> None:\n        \"\"\"Handle a new or updated alert.\"\"\"\n        # Check if similar alert already exists\n        similar_alert = self._find_similar_alert(alert)\n        \n        if similar_alert:\n            # Update existing alert\n            similar_alert.occurrence_count += 1\n            similar_alert.last_occurrence = datetime.utcnow()\n            similar_alert.current_value = alert.current_value\n            \n            # Escalate if needed\n            if similar_alert.occurrence_count > 5 and similar_alert.escalation_level == 0:\n                similar_alert.escalate()\n        else:\n            # Create new alert\n            self.active_alerts[alert.id] = alert\n            self.alert_history.append(alert)\n        \n        # Notify alert handlers\n        for handler in self.alert_handlers:\n            try:\n                await handler(alert)\n            except Exception as e:\n                logger.error(f\"Alert handler failed: {str(e)}\", exc_info=True)\n        \n        logger.info(\n            \"Alert processed\",\n            alert_id=alert.id,\n            title=alert.title,\n            severity=alert.severity.value\n        )\n    \n    def _find_similar_alert(self, alert: Alert) -> Optional[Alert]:\n        \"\"\"Find existing similar alert.\"\"\"\n        for existing_alert in self.active_alerts.values():\n            if (existing_alert.metric_type == alert.metric_type and\n                existing_alert.source_id == alert.source_id and\n                existing_alert.status == AlertStatus.ACTIVE):\n                return existing_alert\n        return None\n    \n    async def _health_check_loop(self) -> None:\n        \"\"\"Background task for agent health monitoring.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                await self._update_agent_health()\n                await asyncio.sleep(self.health_check_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in health check loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(60)\n    \n    async def _update_agent_health(self) -> None:\n        \"\"\"Update health status for all agents.\"\"\"\n        for agent_id, health_status in self.agent_health.items():\n            try:\n                # Update performance score based on metrics\n                if agent_id in self.agent_metrics:\n                    metrics = self.agent_metrics[agent_id]\n                    \n                    # Response time score\n                    if MetricType.RESPONSE_TIME in metrics:\n                        response_times = metrics[MetricType.RESPONSE_TIME].get_recent_values(60)\n                        if response_times:\n                            avg_response_time = statistics.mean(response_times)\n                            health_status.avg_response_time_ms = avg_response_time\n                            \n                            # Score: 100 for <1s, 50 for 5s, 0 for >10s\n                            if avg_response_time < 1000:\n                                health_status.performance_score = 100\n                            elif avg_response_time < 5000:\n                                health_status.performance_score = max(0, 100 - (avg_response_time - 1000) / 40)\n                            else:\n                                health_status.performance_score = max(0, 50 - (avg_response_time - 5000) / 100)\n                    \n                    # Success rate score\n                    if MetricType.SUCCESS_RATE in metrics:\n                        success_rates = metrics[MetricType.SUCCESS_RATE].get_recent_values(60)\n                        if success_rates:\n                            avg_success_rate = statistics.mean(success_rates)\n                            health_status.success_rate_percent = avg_success_rate\n                            health_status.reliability_score = avg_success_rate\n                    \n                    # Error count\n                    if MetricType.ERROR_RATE in metrics:\n                        error_rates = metrics[MetricType.ERROR_RATE].get_recent_values(1440)  # 24 hours\n                        health_status.error_count_24h = sum(error_rates)\n                \n                # Calculate overall health\n                health_status.calculate_overall_health()\n                \n                # Update trends\n                health_status.performance_trend = self._calculate_health_trend(\n                    agent_id, MetricType.RESPONSE_TIME\n                )\n                health_status.reliability_trend = self._calculate_health_trend(\n                    agent_id, MetricType.SUCCESS_RATE\n                )\n                \n            except Exception as e:\n                logger.error(f\"Error updating health for agent {agent_id}: {str(e)}\", exc_info=True)\n    \n    def _calculate_health_trend(self, agent_id: str, metric_type: MetricType) -> str:\n        \"\"\"Calculate health trend for a specific metric.\"\"\"\n        if (agent_id not in self.agent_metrics or\n            metric_type not in self.agent_metrics[agent_id]):\n            return \"stable\"\n        \n        return self.agent_metrics[agent_id][metric_type].calculate_trend()\n    \n    async def _cleanup_loop(self) -> None:\n        \"\"\"Background task for cleaning up old data.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                await self._cleanup_old_alerts()\n                await self._cleanup_old_metrics()\n                await asyncio.sleep(self.cleanup_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in cleanup loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(3600)\n    \n    async def _cleanup_old_alerts(self) -> None:\n        \"\"\"Clean up resolved alerts older than retention period.\"\"\"\n        cutoff_date = datetime.utcnow() - timedelta(days=7)\n        \n        alerts_to_remove = [\n            alert_id for alert_id, alert in self.active_alerts.items()\n            if alert.status == AlertStatus.RESOLVED and alert.resolved_at and alert.resolved_at < cutoff_date\n        ]\n        \n        for alert_id in alerts_to_remove:\n            del self.active_alerts[alert_id]\n        \n        if alerts_to_remove:\n            logger.info(f\"Cleaned up {len(alerts_to_remove)} old resolved alerts\")\n    \n    async def _cleanup_old_metrics(self) -> None:\n        \"\"\"Clean up old metric data points.\"\"\"\n        # Metric series automatically handle cleanup via deque maxlen\n        # This method can be extended for more sophisticated cleanup\n        pass\n    \n    async def _dashboard_update_loop(self) -> None:\n        \"\"\"Background task for updating dashboard data.\"\"\"\n        while not self.shutdown_event.is_set():\n            try:\n                await self._update_dashboard_cache()\n                await asyncio.sleep(30)  # Update dashboard every 30 seconds\n                \n            except Exception as e:\n                logger.error(f\"Error in dashboard update loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(30)\n    \n    async def _update_dashboard_cache(self) -> None:\n        \"\"\"Update cached dashboard data for better performance.\"\"\"\n        # This would update cached dashboard data\n        # Implementation would depend on dashboard requirements\n        pass\n    \n    def add_alert_rule(self, rule: AlertRule) -> None:\n        \"\"\"Add a custom alert rule.\"\"\"\n        self.alert_rules.append(rule)\n        logger.info(f\"Added alert rule: {rule.name}\")\n    \n    def register_alert_handler(self, handler: Callable[[Alert], None]) -> None:\n        \"\"\"Register a handler for alert notifications.\"\"\"\n        self.alert_handlers.append(handler)\n        logger.info(\"Alert handler registered\")\n    \n    async def acknowledge_alert(self, alert_id: str, user_id: str) -> bool:\n        \"\"\"Acknowledge an alert.\"\"\"\n        if alert_id in self.active_alerts:\n            self.active_alerts[alert_id].acknowledge(user_id)\n            logger.info(f\"Alert {alert_id} acknowledged by {user_id}\")\n            return True\n        return False\n    \n    async def resolve_alert(self, alert_id: str) -> bool:\n        \"\"\"Resolve an alert.\"\"\"\n        if alert_id in self.active_alerts:\n            self.active_alerts[alert_id].resolve()\n            logger.info(f\"Alert {alert_id} resolved\")\n            return True\n        return False\n    \n    def get_agent_health(self, agent_id: str) -> Optional[AgentHealthStatus]:\n        \"\"\"Get current health status for an agent.\"\"\"\n        return self.agent_health.get(agent_id)\n    \n    def get_system_overview(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive system overview.\"\"\"\n        # Calculate system-wide statistics\n        total_agents = len(self.agent_health)\n        healthy_agents = len([h for h in self.agent_health.values() if h.overall_health_score >= 80])\n        critical_alerts = len([a for a in self.active_alerts.values() if a.severity == AlertSeverity.CRITICAL])\n        \n        return {\n            \"system_health\": {\n                \"total_agents\": total_agents,\n                \"healthy_agents\": healthy_agents,\n                \"health_percentage\": (healthy_agents / total_agents * 100) if total_agents > 0 else 0,\n                \"uptime_hours\": (datetime.utcnow() - self.start_time).total_seconds() / 3600\n            },\n            \"alerts\": {\n                \"total_active\": len(self.active_alerts),\n                \"critical\": len([a for a in self.active_alerts.values() if a.severity == AlertSeverity.CRITICAL]),\n                \"error\": len([a for a in self.active_alerts.values() if a.severity == AlertSeverity.ERROR]),\n                \"warning\": len([a for a in self.active_alerts.values() if a.severity == AlertSeverity.WARNING])\n            },\n            \"system_metrics\": {\n                metric_type.value: series.aggregations for metric_type, series in self.system_metrics.items()\n            },\n            \"performance_summary\": {\n                \"avg_response_time_ms\": statistics.mean([\n                    h.avg_response_time_ms for h in self.agent_health.values()\n                ]) if self.agent_health else 0,\n                \"overall_success_rate\": statistics.mean([\n                    h.success_rate_percent for h in self.agent_health.values()\n                ]) if self.agent_health else 0\n            }\n        }\n    \n    def get_agent_metrics(\n        self,\n        agent_id: str,\n        metric_types: Optional[List[MetricType]] = None,\n        time_range_minutes: int = 60\n    ) -> Dict[str, Any]:\n        \"\"\"Get metrics for a specific agent.\"\"\"\n        if agent_id not in self.agent_metrics:\n            return {}\n        \n        metrics = self.agent_metrics[agent_id]\n        result = {}\n        \n        target_metrics = metric_types or list(metrics.keys())\n        \n        for metric_type in target_metrics:\n            if metric_type in metrics:\n                series = metrics[metric_type]\n                result[metric_type.value] = {\n                    \"current\": series.aggregations.get(\"current\", 0),\n                    \"average\": series.aggregations.get(\"avg\", 0),\n                    \"min\": series.aggregations.get(\"min\", 0),\n                    \"max\": series.aggregations.get(\"max\", 0),\n                    \"trend\": series.calculate_trend(time_range_minutes),\n                    \"recent_values\": series.get_recent_values(time_range_minutes)[-20:]  # Last 20 values\n                }\n        \n        return result\n    \n    async def get_system_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive system status.\"\"\"\n        return {\n            \"status\": \"running\" if self.is_running else \"stopped\",\n            \"monitored_agents\": len(self.agent_health),\n            \"active_alerts\": len(self.active_alerts),\n            \"alert_rules\": len(self.alert_rules),\n            \"uptime_seconds\": (datetime.utcnow() - self.start_time).total_seconds(),\n            \"system_overview\": self.get_system_overview(),\n            \"buffer_status\": {\n                \"total_buffers\": len(self.metric_buffer),\n                \"total_buffered_points\": sum(len(points) for points in self.metric_buffer.values())\n            }\n        }\n    \n    async def shutdown(self) -> None:\n        \"\"\"Gracefully shutdown the monitoring system.\"\"\"\n        logger.info(\"Shutting down agent monitoring system\")\n        \n        self.shutdown_event.set()\n        \n        # Flush all buffers before shutdown\n        await self._flush_all_buffers()\n        \n        # Cancel background tasks\n        tasks = [self.collection_task, self.evaluation_task, self.health_task, \n                self.cleanup_task, self.dashboard_task]\n        for task in tasks:\n            if task:\n                task.cancel()\n        \n        self.is_running = False\n        logger.info(\"Agent monitoring system shutdown complete\")\n\n\n# Global monitoring system instance\nmonitoring_system = AgentMonitoringSystem()\n\n\nasync def get_monitoring_system() -> AgentMonitoringSystem:\n    \"\"\"Get the global monitoring system instance.\"\"\"\n    if not monitoring_system.is_running:\n        await monitoring_system.initialize()\n    return monitoring_system