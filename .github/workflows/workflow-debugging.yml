name: Workflow Debugging Assistant

on:
  workflow_run:
    workflows: ['*']
    types: [completed]
  workflow_dispatch:
    inputs:
      workflow-run-id:
        description: 'Workflow run ID to debug'
        required: false
      failure-analysis-depth:
        description: 'Depth of failure analysis'
        required: false
        default: 'standard'
        type: choice
        options:
          - basic
          - standard
          - deep

env:
  MAX_LOG_SIZE: 50000
  ANALYSIS_TIMEOUT: 600
  DEBUG_ISSUE_LABELS: 'workflow-debug,automated,needs-attention'

permissions:
  actions: read
  contents: read
  issues: write
  pull-requests: write

jobs:
  detect-failures:
    if: github.event.workflow_run.conclusion == 'failure' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    outputs:
      should-debug: ${{ steps.failure-check.outputs.should-debug }}
      workflow-name: ${{ steps.failure-check.outputs.workflow-name }}
      failure-count: ${{ steps.failure-check.outputs.failure-count }}
    steps:
      - name: Check failure conditions
        id: failure-check
        uses: actions/github-script@v7
        with:
          script: |
            let workflowRunId, workflowName, shouldDebug = false;
            let failureCount = 0;

            if (context.eventName === 'workflow_run') {
              workflowRunId = context.payload.workflow_run.id;
              workflowName = context.payload.workflow_run.name;
              
              // Check if this is a recurring failure
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: context.payload.workflow_run.workflow_id,
                status: 'failure',
                per_page: 5
              });
              
              failureCount = runs.data.workflow_runs.length;
              
              // Debug if:
              // 1. This is the 2nd consecutive failure
              // 2. Or if it's a critical workflow (contains 'deploy', 'release', 'security')
              const criticalKeywords = ['deploy', 'release', 'security', 'production'];
              const isCritical = criticalKeywords.some(keyword => 
                workflowName.toLowerCase().includes(keyword)
              );
              
              shouldDebug = failureCount >= 2 || isCritical;
              
            } else {
              // Manual trigger
              workflowRunId = context.payload.inputs['workflow-run-id'];
              shouldDebug = true;
              failureCount = 1;
            }

            core.setOutput('should-debug', shouldDebug);
            core.setOutput('workflow-name', workflowName || 'Manual Debug');
            core.setOutput('failure-count', failureCount);
            core.setOutput('workflow-run-id', workflowRunId);

            console.log(`Should debug: ${shouldDebug}, Failure count: ${failureCount}`);

  download-logs:
    needs: detect-failures
    if: needs.detect-failures.outputs.should-debug == 'true'
    runs-on: ubuntu-latest
    outputs:
      logs-available: ${{ steps.download.outputs.logs-available }}
      error-patterns: ${{ steps.analyze-patterns.outputs.error-patterns }}
    steps:
      - name: Setup debugging environment
        run: |
          mkdir -p debug-workspace/{logs,analysis,reports}
          pip install pyyaml requests

      - name: Download workflow logs
        id: download
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            try {
              const workflowRunId = '${{ needs.detect-failures.outputs.workflow-run-id }}' || 
                                   context.payload.workflow_run.id;
              
              // Download logs
              const logs = await github.rest.actions.downloadWorkflowRunLogs({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: workflowRunId
              });
              
              // Save logs to file
              fs.writeFileSync('debug-workspace/logs/workflow-logs.zip', Buffer.from(logs.data));
              
              core.setOutput('logs-available', 'true');
              console.log('✅ Logs downloaded successfully');
              
            } catch (error) {
              console.log(`❌ Failed to download logs: ${error.message}`);
              core.setOutput('logs-available', 'false');
              
              // Create minimal error info
              fs.writeFileSync('debug-workspace/logs/error-info.txt', 
                `Error downloading logs: ${error.message}\n` +
                `Workflow: ${{ needs.detect-failures.outputs.workflow-name }}\n` +
                `Run ID: ${workflowRunId}\n`
              );
            }

      - name: Extract and analyze logs
        if: steps.download.outputs.logs-available == 'true'
        run: |
          cd debug-workspace/logs

          # Extract logs
          unzip -q workflow-logs.zip || echo "Failed to extract logs"

          # Find log files
          find . -name "*.txt" -type f | head -20 > log-files.list

          # Create combined log file for analysis
          echo "=== COMBINED WORKFLOW LOGS ===" > combined-logs.txt
          while read -r logfile; do
            echo "=== LOG: $logfile ===" >> combined-logs.txt
            head -n 1000 "$logfile" >> combined-logs.txt 2>/dev/null || echo "Could not read $logfile"
            echo "" >> combined-logs.txt
          done < log-files.list

          # Truncate if too large
          if [ $(wc -c < combined-logs.txt) -gt ${{ env.MAX_LOG_SIZE }} ]; then
            head -c ${{ env.MAX_LOG_SIZE }} combined-logs.txt > combined-logs-truncated.txt
            mv combined-logs-truncated.txt combined-logs.txt
            echo "... (truncated due to size)" >> combined-logs.txt
          fi

      - name: Analyze error patterns
        id: analyze-patterns
        run: |
          cd debug-workspace/logs

          # Common error patterns
          cat > error-patterns.txt << 'EOF'
          # Error pattern analysis
          npm_errors=$(grep -i "npm.*error\|npm.*fail" combined-logs.txt | wc -l)
          python_errors=$(grep -i "python.*error\|traceback\|modulenotfounderror" combined-logs.txt | wc -l)
          docker_errors=$(grep -i "docker.*error\|container.*fail" combined-logs.txt | wc -l)
          network_errors=$(grep -i "network.*error\|connection.*timeout\|dns.*fail" combined-logs.txt | wc -l)
          permission_errors=$(grep -i "permission.*denied\|access.*forbidden" combined-logs.txt | wc -l)
          timeout_errors=$(grep -i "timeout\|timed out" combined-logs.txt | wc -l)

          # Output patterns found
          echo "NPM Errors: $npm_errors"
          echo "Python Errors: $python_errors" 
          echo "Docker Errors: $docker_errors"
          echo "Network Errors: $network_errors"
          echo "Permission Errors: $permission_errors"
          echo "Timeout Errors: $timeout_errors"
          EOF

          # Execute pattern analysis
          bash error-patterns.txt > pattern-summary.txt

          # Create JSON output for GitHub Actions
          cat pattern-summary.txt | jq -R -s 'split("\n") | map(select(. != "")) | map(split(": ") | {(.[0]): (.[1] | tonumber)}) | add' > patterns.json

          # Set output
          echo "error-patterns=$(cat patterns.json)" >> $GITHUB_OUTPUT

  ai-analysis:
    needs: [detect-failures, download-logs]
    if: needs.detect-failures.outputs.should-debug == 'true' && needs.download-logs.outputs.logs-available == 'true'
    runs-on: ubuntu-latest
    outputs:
      analysis-result: ${{ steps.ai-debug.outputs.analysis-result }}
    steps:
      - name: Setup Python environment
        run: |
          pip install requests openai anthropic
          mkdir -p debug-workspace/analysis

      - name: Prepare analysis context
        run: |
          cd debug-workspace

          # Create analysis context file
          cat > analysis/context.json << 'EOF'
          {
            "workflow_name": "${{ needs.detect-failures.outputs.workflow-name }}",
            "failure_count": ${{ needs.detect-failures.outputs.failure-count }},
            "repository": "${{ github.repository }}",
            "error_patterns": ${{ needs.download-logs.outputs.error-patterns }},
            "analysis_depth": "${{ github.event.inputs.failure-analysis-depth || 'standard' }}",
            "timestamp": "${{ github.event.workflow_run.created_at || github.event.head_commit.timestamp }}"
          }
          EOF

      - name: AI-powered failure analysis
        id: ai-debug
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        run: |
          cd debug-workspace

          # Create analysis script
          cat > analysis/analyze_failure.py << 'EOF'
          import json
          import os
          import requests
          import sys
          from datetime import datetime

          def get_api_key_and_url():
              if os.getenv('DEEPSEEK_API_KEY'):
                  return os.getenv('DEEPSEEK_API_KEY'), "https://api.deepseek.com/v1/chat/completions", "deepseek-coder"
              elif os.getenv('OPENAI_API_KEY'):
                  return os.getenv('OPENAI_API_KEY'), "https://api.openai.com/v1/chat/completions", "gpt-4"
              else:
                  print("No API key available")
                  sys.exit(1)

          def analyze_workflow_failure():
              # Load context
              with open('analysis/context.json', 'r') as f:
                  context = json.load(f)
              
              # Load logs if available
              log_content = ""
              if os.path.exists('logs/combined-logs.txt'):
                  with open('logs/combined-logs.txt', 'r') as f:
                      log_content = f.read()[:8000]  # Truncate for API limits
              
              api_key, api_url, model = get_api_key_and_url()
              
              # Create analysis prompt
              system_prompt = """You are a GitHub Actions workflow debugging expert. Analyze the failed workflow and provide:
              
              1. **Root Cause Analysis**: Identify the primary reason for failure
              2. **Specific Fix Recommendations**: Actionable steps to resolve the issue
              3. **Prevention Strategies**: How to prevent similar failures
              4. **Urgency Level**: Critical, High, Medium, or Low
              5. **Estimated Fix Time**: Hours needed to resolve
              
              Format your response with clear headings and actionable recommendations."""
              
              user_prompt = f"""
              Workflow Debug Request:
              - **Workflow**: {context['workflow_name']}
              - **Failure Count**: {context['failure_count']}
              - **Repository**: {context['repository']}
              - **Analysis Depth**: {context['analysis_depth']}
              
              **Error Patterns Detected**:
              {json.dumps(context['error_patterns'], indent=2)}
              
              **Log Sample**:
              ```
              {log_content}
              ```
              
              Please provide a comprehensive analysis with specific, actionable recommendations.
              """
              
              headers = {
                  "Authorization": f"Bearer {api_key}",
                  "Content-Type": "application/json"
              }
              
              payload = {
                  "model": model,
                  "messages": [
                      {"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}
                  ],
                  "temperature": 0.1,
                  "max_tokens": 4000
              }
              
              try:
                  response = requests.post(api_url, headers=headers, json=payload, timeout=60)
                  response.raise_for_status()
                  
                  result = response.json()
                  analysis = result['choices'][0]['message']['content']
                  
                  # Save analysis
                  with open('analysis/ai-analysis.md', 'w') as f:
                      f.write(f"# Workflow Debug Analysis\n\n")
                      f.write(f"**Generated**: {datetime.utcnow().isoformat()}Z\n\n")
                      f.write(analysis)
                  
                  # Create summary for GitHub Actions
                  summary = {
                      "status": "success",
                      "analysis": analysis,
                      "model_used": model,
                      "timestamp": datetime.utcnow().isoformat()
                  }
                  
                  with open('analysis/result.json', 'w') as f:
                      json.dump(summary, f, indent=2)
                  
                  return analysis
                  
              except Exception as e:
                  error_analysis = f"AI analysis failed: {str(e)}\n\n"
                  error_analysis += "## Manual Debug Checklist\n"
                  error_analysis += "1. Check workflow syntax and configuration\n"
                  error_analysis += "2. Verify environment variables and secrets\n"
                  error_analysis += "3. Review recent dependency changes\n"
                  error_analysis += "4. Check for resource constraints\n"
                  error_analysis += "5. Validate permissions and access rights\n"
                  
                  summary = {
                      "status": "fallback",
                      "analysis": error_analysis,
                      "error": str(e),
                      "timestamp": datetime.utcnow().isoformat()
                  }
                  
                  with open('analysis/result.json', 'w') as f:
                      json.dump(summary, f, indent=2)
                  
                  return error_analysis

          if __name__ == "__main__":
              result = analyze_workflow_failure()
              print("Analysis completed")
          EOF

          # Run analysis
          timeout ${{ env.ANALYSIS_TIMEOUT }} python analysis/analyze_failure.py

          # Set output
          if [ -f "analysis/result.json" ]; then
            echo "analysis-result=$(cat analysis/result.json | jq -c .)" >> $GITHUB_OUTPUT
          else
            echo "analysis-result={\"status\":\"failed\",\"analysis\":\"Analysis could not be completed\"}" >> $GITHUB_OUTPUT
          fi

  create-debug-issue:
    needs: [detect-failures, download-logs, ai-analysis]
    if: always() && needs.detect-failures.outputs.should-debug == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Create debugging issue
        uses: actions/github-script@v7
        with:
          script: |
            const workflowName = '${{ needs.detect-failures.outputs.workflow-name }}';
            const failureCount = '${{ needs.detect-failures.outputs.failure-count }}';
            const analysisResult = JSON.parse('${{ needs.ai-analysis.outputs.analysis-result || '{}' }}');

            // Create issue title
            const title = `🔧 Workflow Debug: ${workflowName} (${failureCount} failures)`;

            // Create issue body
            let body = `## Workflow Debug Report\n\n`;
            body += `**Workflow**: ${workflowName}\n`;
            body += `**Failure Count**: ${failureCount}\n`;
            body += `**Run ID**: ${{ github.run_id }}\n`;
            body += `**Triggered**: ${new Date().toISOString()}\n\n`;

            if (analysisResult.analysis) {
              body += `## 🤖 AI Analysis\n\n`;
              body += analysisResult.analysis;
              body += `\n\n*Analysis provided by: ${analysisResult.model_used || 'AI Assistant'}*\n\n`;
            }

            body += `## 🔍 Debug Information\n\n`;
            body += `- **Error Patterns**: ${{ needs.download-logs.outputs.error-patterns }}\n`;
            body += `- **Logs Available**: ${{ needs.download-logs.outputs.logs-available }}\n`;
            body += `- **Analysis Status**: ${analysisResult.status || 'not-available'}\n\n`;

            body += `## 📋 Next Steps\n\n`;
            body += `- [ ] Review the AI analysis recommendations\n`;
            body += `- [ ] Check the workflow configuration\n`;
            body += `- [ ] Verify environment setup and dependencies\n`;
            body += `- [ ] Test fixes in a separate branch\n`;
            body += `- [ ] Update documentation if needed\n\n`;

            body += `---\n*This issue was automatically created by the Workflow Debugging Assistant*`;

            // Check if similar issue exists
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'workflow-debug',
              state: 'open'
            });

            const similarIssue = existingIssues.data.find(issue => 
              issue.title.includes(workflowName)
            );

            if (similarIssue) {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: similarIssue.number,
                body: `## 🔄 Additional Failure Detected\n\n${body}`
              });
              console.log(`Updated existing issue #${similarIssue.number}`);
            } else {
              // Create new issue
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: '${{ env.DEBUG_ISSUE_LABELS }}'.split(',')
              });
              console.log(`Created new debug issue #${issue.data.number}`);
            }

  workflow-summary:
    needs: [detect-failures, download-logs, ai-analysis, create-debug-issue]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Create workflow summary
        run: |
          echo "## 🔧 Workflow Debug Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ needs.detect-failures.outputs.workflow-name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Should Debug**: ${{ needs.detect-failures.outputs.should-debug }}" >> $GITHUB_STEP_SUMMARY
          echo "**Failure Count**: ${{ needs.detect-failures.outputs.failure-count }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "- 📥 Log Download: ${{ needs.download-logs.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- 🤖 AI Analysis: ${{ needs.ai-analysis.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- 📝 Issue Creation: ${{ needs.create-debug-issue.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.detect-failures.outputs.should-debug }}" == "true" ]; then
            echo "✅ Debug issue has been created with detailed analysis and recommendations." >> $GITHUB_STEP_SUMMARY
          else
            echo "ℹ️ Workflow failure did not meet debug criteria." >> $GITHUB_STEP_SUMMARY
          fi
